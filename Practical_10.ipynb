{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical X\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "6-11.01.2025\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will evaluate the performance of the transformer model we trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Autoregressive Generation\n",
    "\n",
    "In order to generate a translation we will use the autoregressive property of the transformer model. We will use the following procedure to generate a translation:\n",
    "\n",
    "1. Encode the source sentence using the encoder.\n",
    "2. Initialize the decoder with the encoded source sentence.\n",
    "3. Generate the first token of the translation by passing the start of text token through the decoder.\n",
    "4. Pass the generated token through the decoder to generate the next token and repeat until the end of text token is generated.\n",
    "\n",
    "#### 1.1. Greedy Decoding\n",
    "\n",
    "The simplest way to generate a translation is to use greedy decoding. In greedy decoding we simply select the token with the highest probability at each step.\n",
    "\n",
    "### 2. Evaluation\n",
    "\n",
    "In order to evaluate the performance of the model we will use the BLEU score. The BLEU score is a metric that measures the similarity between two sentences. See the [huggingface evaluate documentation](https://huggingface.co/spaces/evaluate-metric/bleu) for more information on the BLEU score, as well as details on using the metric in huggingface evaluate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Implement the autoregressive generation procedure described above using your transformer model. (Using greedy decoding, remember to add a maximum length to the generation procedure to prevent infinite generation.)\n",
    "2. Generate translations for the test set (or a subset of the test set) of WMT17 German-English.\n",
    "3. Evaluate the BLEU score of your model on the test set (or a subset of the test set) of WMT17 German-English.\n",
    "4. Evaluate some of the translations generated by your model. Do they make sense? What are some of the errors made by your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c8e9c54ce28429",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonmarkwart/miniconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/leonmarkwart/miniconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/leonmarkwart/miniconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <AF61AFB3-B2B8-3E0A-ABE9-D478E0B733F9> /Users/leonmarkwart/miniconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/leonmarkwart/miniconda3/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from modelling.model import Transformer\n",
    "from train import TransformerModel\n",
    "import torch\n",
    "from dataset import get_costum_dataset\n",
    "import evaluate\n",
    "\n",
    "\n",
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"modelling/bpe_v=30016_l=64\")\n",
    "# pytorch lightning takes care of loading configuration and checkpoint\n",
    "model = TransformerModel.load_from_checkpoint(\"lightning_logs/without_source_BOS/checkpoints/epoch=9-step=132074.ckpt\")\n",
    "model.eval()\n",
    "print(model.device)\n",
    "src_input = \"Mein Name ist Leon und ich bin ein Student.\"\n",
    "tgt_input = \"My name is Leon and I am a student.\"\n",
    "tokenizer.model_max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3371197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mein Name ist Leon und ich bin ein Student.\n",
      "torch.Size([1, 64])\n",
      "Mein Name ist Leon und ich bin ein Student.[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "a \t 12.54144287109375 %\n",
      " name \t 72.57445526123047 %\n",
      " is \t 19.266252517700195 %\n",
      " Leon \t 11.922411918640137 %\n",
      " and \t 67.21080017089844 %\n",
      ". \t 20.942171096801758 %\n",
      " is \t 6.50723934173584 %\n",
      ". \t 4.774482250213623 %\n",
      " to \t 4.989753246307373 %\n",
      " a \t 7.778448581695557 %\n",
      ". \t 6.572515487670898 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a name is Leon and. is. to a.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_translate(src_input_sentence, model, tokenizer, first_token_bos=False):\n",
    "    print(src_input_sentence)\n",
    "    src_input = tokenizer(src_input_sentence, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=tokenizer.model_max_length +1)['input_ids']\n",
    "    if first_token_bos:\n",
    "        src_input = src_input[:, :-1]\n",
    "    else:\n",
    "        src_input = src_input[:, 1:]\n",
    "    print(src_input.shape)\n",
    "    print(tokenizer.decode(src_input[0]))\n",
    "    #shift source input one to the left and replace the last token with the end padding token\n",
    "    src_input = src_input[:, 1:]\n",
    "    tgt_input = torch.zeros_like(src_input)\n",
    "    # set the first token to the start of sentence token\n",
    "    tgt_input[:, 0] = 1 #tokenizer.\n",
    "    for i in range(1, tokenizer.model_max_length):\n",
    "        output = model(src_input.to(model.device), tgt_input.to(model.device)).softmax(dim=-1)\n",
    "        score, output_token = output[:, i].max(dim=-1)\n",
    "        if output_token.item() in [tokenizer.convert_tokens_to_ids('[PAD]'), tokenizer.convert_tokens_to_ids('[EOS]')]:\n",
    "            break\n",
    "        tgt_output = tgt_input.clone()\n",
    "        tgt_output[:, i] = output_token\n",
    "        print(tokenizer.decode(output_token), '\\t', (score*100).item(), '%')\n",
    "        tgt_input = tgt_output\n",
    "\n",
    "    return tokenizer.decode(tgt_input[0], skip_special_tokens=True)\n",
    "\n",
    "greedy_translate(src_input, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582693d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TransformerModel.predict_step() missing 1 required positional argument: 'batch_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m src_input \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_input\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m tgt_input \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict_step(sample)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerModel.predict_step() missing 1 required positional argument: 'batch_idx'"
     ]
    }
   ],
   "source": [
    "test_ds = get_costum_dataset(\"test\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_ds:\n",
    "        src_input = sample['src_input']\n",
    "        tgt_input = sample['tgt_output']\n",
    "        model.predict_step(sample)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
