{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical II\n",
    "Carel van Niekerk\n",
    "\n",
    "16-23.10.2023\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will explore the basics of the transformer model namely, embedding input sequences and the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741b35bb534bafe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the [tutorial](https://vgel.me/posts/handmade-transformer/), up to the end of the section on designing the attention head. (Feel free to use the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper as an extra resource)\n",
    "2. Make sure you are understand the embeddings of the input. Explain why we need the position of input characters in the embedding.\n",
    "3. Implement the attention mechanism. Please implement the attention mechanism as a pytorch module as you will need this later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24533dfe6da00b63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
