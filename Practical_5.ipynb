{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical IV\n",
    "Carel van Niekerk\n",
    "\n",
    "13-20.11.2023\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will explore the translation task, the dataset for translation and cleaning up and preparing the data for training a model. In the sessions this week we will discuss how to create the tokenizer class for pre-training and how to create dataset and dataloader objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf904ffe07eb02b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Translation Task\n",
    "\n",
    "The translation task is a sequence to sequence task. The input is a sequence of words in one language and the output is a sequence of words in another language. The input and output sequences are not necessarily the same length. The input sequence is usually referred to as the source sequence and the output sequence as the target sequence.\n",
    "\n",
    "#### 1.1. Representing the translation task numerically\n",
    "\n",
    "Before building a neural network model for machine translation, it's crucial to understand how text data, a series of characters or words, is converted into a format that can be processed numerically by neural network models. Here's a brief overview:\n",
    "\n",
    "##### 1. Tokenization:\n",
    "\n",
    "This is the first step, where a piece of text is divided into smaller chunks, called tokens. For now we assume these tokens are words. For example, *\"Welcome to this practical series!\"* can be tokenized into `['welcome', 'to', 'this', 'practical', 'series', '!']`.\n",
    "\n",
    "##### 2. Building a Vocabulary:\n",
    "\n",
    "Once texts are tokenized, a vocabulary is constructed. This is a unique list of words found in the dataset. From our example, the vocabulary would be: `{'welcome', 'to', 'this', 'practical', 'series', '!'}`.\n",
    "\n",
    "##### 3.Encoding Words as Numbers:\n",
    "\n",
    "Each unique word in the vocabulary is assigned a unique numerical identifier. This allows us to convert textual data into a numerical format. Using our example: *'welcome'* might be represented as '0', *'to'* as '1' and so on. This results in the numerical representation `[0, 1, 2, 3, 4, 5]` for the sentence *\"Welcome to this practical series!\"*.\n",
    "\n",
    "By transforming text data into numerical form, sequence to sequence neural network models can process and learn from the data, enabling them to perform tasks such as machine translation.\n",
    "\n",
    "#### 1.2. The Dataset\n",
    "We will use the [WMT 17 German to English dataset](https://huggingface.co/datasets/wmt17/viewer/de-en/train) for this practical. This dataset contains 5.9 million sentence pairs. The dataset is available through the Huggingface datasets package and can be loaded as follows:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wmt17\", \"de-en\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f7f8c05af7cc3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Cleaning and Preparing the Data\n",
    "\n",
    "#### 2.1. Cleaning the Data\n",
    "\n",
    "Before we can use this dataset for training a model, we need to clean it. This involves removing sentences that are too long or too short, removing sentences that contain too many unknown tokens and removing sentences that contain too many non-alphabetic characters. We will also convert all sentences to lowercase.\n",
    "\n",
    "We will apply the following set of operations to clean the data:\n",
    "\n",
    "- Remove any non-UTF8 characters, URL's or HTML tags.\n",
    "- Remove characters not whitelisted to avoid injecting unnecessary characters into the model vocab. We will use the whitelist `\"abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?()[]{}:;-&$@#%£€/\\|_+*¥\"` for this.\n",
    "- Remove sentences that are too long or too short. We will use a minimum length of 5 and a maximum length of 64 for this.\n",
    "- Remove translation-pairs where the ratio between the source and target sentence is too large.\n",
    "- Remove sentences that contain too many unknown tokens.\n",
    "\n",
    "#### 2.2. Building the Vocabulary of the Model\n",
    "\n",
    "Once the data is cleaned we can build the vocabulary of the model. The vocabulary should consists of the top V most frequent words in the dataset. We will use a vocabulary size of V=50000 for this practical (in addition we will have a separate vocabulary for the source and target languages. The vocabulary should also contain special tokens for padding, unknown tokens and the start and end of a sentence `[PAD], [UNK], [BOS], [EOS]`.\n",
    "\n",
    "#### 2.3. Encoding the Data\n",
    "\n",
    "Once the vocabulary is built, we can encode the data. This involves converting the text data into numerical form using the vocabulary. Here it is important to consider that in neural networks data is processed batches of sequences simultaneously for computational efficiency. However, real-world text sequences vary in length, and this presents a challenge: neural networks require inputs of a consistent shape and size.\n",
    "\n",
    "This is where padding tokens come into play. Padding ensures that all sequences in a batch share the same length by appending special <PAD> tokens to shorter sequences until they match the length of the longest sequence in the batch. This uniformity is crucial for matrix (and tensor) operations inside the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741b35bb534bafe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Write a data cleaning function for the dataset.\n",
    "2. Create a tokenizer class, this class should be based on the huggingface BPE tokenizer used in Practical 4. (Hint: learn the vocabulary using the BPETokenizer class, convert the vocabulary and merges dict to the format required by the GPT2Tokenizer class and then create a GPT2Tokenizer object using the from_pretrained method).\n",
    "3. Write a torch dataset class to encode the dataset and store it. This class will be used to create dataloaders for training your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24533dfe6da00b63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
