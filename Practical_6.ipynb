{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical VI\n",
    "Carel van Niekerk\n",
    "\n",
    "20-27.11.2023\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will explore word embedding layers. We will explore the embedding layer, as well as the positional embedding layer (recall in practical 2 we discussed the importance of encoding the position of an element in a sequence). We will discuss the positional encodings and proofs in the practical sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Word Embeddings\n",
    "\n",
    "Word embeddings are a way of representing words as vectors. These vectors are learned during training, and are used to represent words in a way that captures their meaning. This is useful for many NLP tasks, as it allows us to represent words in a way that is more useful for machine learning models.\n",
    "\n",
    "Mathematically the word embedding layer is represented as:\n",
    "\n",
    " ${\\tt Emb}(\\mathbf{x}) = \\mathcal{1} (\\mathbf{x}) \\mathbf{E}$ \n",
    "\n",
    "where $\\mathbf{x}$ is a vector of word indices, $\\mathcal{1}$ is a one-hot encoding function, and $\\mathbf{E}$ is a matrix of word embeddings. The one-hot encoding function is a function that takes a vector of word indices, and returns a matrix where each row is a one-hot encoding of the corresponding word index. The word embedding matrix is a matrix where each row is the word embedding of the corresponding word index.\n",
    "\n",
    "The word embedding layer acts as a lookup table, where each row of the matrix is a word embedding. Multiplying the one-hot matrix with the embedding matrix is equivalent to selecting the word embedding of the word index. We will utilise the pytorch `Embedding` layer to implement this. (See [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) for more details.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3db3becedbe2f606"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the positional encoding layer proposed in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762). Prove the properties of the positional encodings presented (i.e. for a fixed offset k the positional encodings $PE_{t+k}$ can be represented as a linear function of $PE_t$ and The wavelengths form a geometric progression from 2π to 10000*2π).\n",
    "2. Implement the positional encoding layer in pytorch.\n",
    "3. Implement the word embedding layer in pytorch.\n",
    "4. Test your positional encoding layer using the provided tests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a20b8711fe743b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f6c8e9c54ce28429"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
