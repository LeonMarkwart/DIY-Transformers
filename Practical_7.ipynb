{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical VII\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "25-29.11.2024\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will combine the word embedding layer, positional encoding layer, and encoder and decoder layers from previous practicals to implement a transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the model in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762). Write down the structure of the proposed model.\n",
    "2. Study the section on word embeddings and pay close attention to the parameter sharing. Explain the benefits of parameter sharing in the transformer model.\n",
    "3. Based on your implementations of all the components, implement a transformer model. Use the pytorch `nn.Module` class to implement the model. Your model should be configurable with the following parameters:\n",
    "    - `vocab_size`: The size of the vocabulary\n",
    "    - `d_model`: The dimensionality of the embedding layer\n",
    "    - `n_heads`: The number of heads in the multi-head attention layers\n",
    "    - `num_encoder_layers`: The number of encoder layers\n",
    "    - `num_decoder_layers`: The number of decoder layers\n",
    "    - `dim_feedforward`: The dimensionality of the feedforward layer\n",
    "    - `dropout`: The dropout probability\n",
    "    - `max_len`: The maximum length of the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c42fc",
   "metadata": {},
   "source": [
    "___\n",
    "## 1: Structure of the Transformer Model\n",
    "#TODO: Write down the structure of the proposed model in the paper \"Attention is all you need\".\n",
    "## 2: Advantages of Byte Pair Encoding (BPE)\n",
    "\n",
    "In the Transformer model, the same BPE vocabulary is shared between the source (input) and target (output) languages. This means that the tokenization process for both the source and target texts uses the same set of subwords, which provides several advantages:\n",
    "\n",
    "1. **Efficiency**: A shared vocabulary reduces the overall number of parameters in the embedding layers, as the same embeddings are used for both the source and target.\n",
    "2. **Consistency**: Using a shared vocabulary ensures that the model can easily relate similar or identical subwords between the source and target languages. This is particularly useful in cases where the source and target languages share common roots or loanwords.\n",
    "3. **Simplification**: A shared vocabulary simplifies the training process, as it removes the need to maintain two separate vocabularies.\n",
    "\n",
    "However, a shared vocabulary can also present challenges, such as the need to balance the representation of both languages within a single vocabulary, which might be difficult if the languages are very different or have vastly different character sets.\n",
    "\n",
    "## 3: Implementing the Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4122172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelling.attention import MultiHeadAttention\n",
    "from modelling.feedforward import PointWiseFeedForward\n",
    "from modelling.layernorm import LayerNorm\n",
    "from modelling.functional import TransformerDecoderLayer, BaseTransformerLayer\n",
    "from modelling.positional_encoding import PositionalEncoding\n",
    "\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6dab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 d_model, \n",
    "                 n_heads, \n",
    "                 num_encoder_layers, \n",
    "                 num_decoder_layers, \n",
    "                 dim_feedforward, \n",
    "                 dropout, \n",
    "                 max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len, dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.transformer_encoder = nn.ModuleDict({\n",
    "            f\"encoder_layer_{i}\": BaseTransformerLayer(d_model, n_heads, dim_feedforward, dropout) \n",
    "            for i in range(num_encoder_layers)\n",
    "        })\n",
    "\n",
    "        self.transformer_decoder = nn.ModuleDict({\n",
    "            f\"decoder_layer_{i}\": TransformerDecoderLayer(d_model, n_heads, dim_feedforward, dropout) \n",
    "            for i in range(num_decoder_layers)\n",
    "        })\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, vocab_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.src_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        src = self.dropout(src)\n",
    "        for encoder_layer in self.transformer_encoder.values():\n",
    "            src = encoder_layer(src)\n",
    "\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        tgt = self.dropout(tgt)\n",
    "        for decoder_layer in self.transformer_decoder.values():\n",
    "            tgt = decoder_layer(tgt, src)\n",
    "\n",
    "        return self.head(tgt)\n",
    "\n",
    "# Example usage\n",
    "transformer = Transformer(100, 512, 8, 6, 6, 2048, 0.1, 5000)\n",
    "x_in = torch.randint(0, 100, (32, 10))\n",
    "y_in = torch.randint(0, 100, (32, 10))\n",
    "out = transformer(x_in, y_in)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
