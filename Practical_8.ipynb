{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical VIII\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "2-6.12.2024\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will implement the learning rate scheduler as well as initialise the optimiser to train our transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Learning rate scheduler\n",
    "\n",
    "The learning rate scheduler is used to adjust the learning rate during training. The learning rate scheduler is called every training step and returns the learning rate for that step. In a transformer model the learning rate scheduler is important because the model is trained for a long time and the learning rate needs to be adjusted to ensure that the model converges to a good solution.\n",
    "\n",
    "### 2. Optimiser\n",
    "\n",
    "In this practical we will use the AdamW optimiser. The AdamW optimiser is a variant of the Adam optimiser that uses weight decay to regularise the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the learning rate scheduler used in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762).\n",
    "2. Implement the learning rate scheduler. It is important that your scheduler class has the same interface as the pytorch learning rate scheduler classes, that is, it should have a `step()` method that updates the learning rate and a `get_lr()` method that returns the learning rate for the current step.\n",
    "3. Study the AdamW optimiser used. Write down the update equations and explain the reasoning behind the bias correction and decoupled weight decay.\n",
    "4. Initialise a AdamW optimiser for your transformer model. It is important to not use weight decay on the bias and layer normalisation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8e9c54ce28429",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "class TransformerLRScheduler(LambdaLR):\n",
    "    def __init__(self, \n",
    "                 optimizer: Optimizer, \n",
    "                 d_model: int, \n",
    "                 warmup_steps: int = 4000) -> None:\n",
    "        \n",
    "        super().__init__(optimizer, self.lr_lambda)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def lr_lambda(self, step: int) -> float:\n",
    "        step = step if step != 0 else 1\n",
    "        return self.d_model ** (-0.5) * min(step ** (-0.5), step * self.warmup_steps ** (-1.5))\n",
    "    \n",
    "    # other methods are inherited from LambdaLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579370b1",
   "metadata": {},
   "source": [
    "# AdamW optimiser update equations\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot (g_t)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\text{lr} \\cdot \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\text{weight\\_decay} \\cdot \\theta_{t-1} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise AdamW optimiser\n",
    "model = ...  # your transformer model\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=1e-3)\n",
    "\n",
    "# Initialise learning rate scheduler\n",
    "scheduler = TransformerLRScheduler(optimizer, d_model=512, warmup_steps=4000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
