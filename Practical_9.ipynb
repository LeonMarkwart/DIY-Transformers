{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical IX\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "9-20.12.2024\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will implement the training script train the transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Essentials of the training script\n",
    "\n",
    "In the training script the prepared dataset is used to train a model instance. Before training the model, the instance should be initialised, a dataloader should be created from the dataset and the loss function, optimiser and learning rate scheduler should be initialised. \n",
    "\n",
    "#### 1.1. Dataloaders\n",
    "\n",
    "The dataloader is a pytorch class that is used to load data from the dataset in batches. The dataloader is initialised with the dataset and the batch size. The dataloader is then used to iterate over the dataset in batches. The dataloader is used in the training loop to load the data for each batch.\n",
    "\n",
    "The dataloader for the training data is initialised as follows:\n",
    "\n",
    "```python\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "```\n",
    "\n",
    "#### 1.2. Training the model\n",
    "\n",
    "Once the model, dataloader and loss function are initialised, the model can be trained. The training loop iterates over the batches in the dataloader and performs the following steps:\n",
    "\n",
    "1. Load the data for the batch.\n",
    "2. Perform a forward pass through the model.\n",
    "3. Calculate the loss.\n",
    "4. Perform a backward pass through the model.\n",
    "5. Update the model parameters.\n",
    "6. Update the learning rate.\n",
    "7. Repeat for the next batch.\n",
    "8. Repeat for the next epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Initialise a small version of your transformer model (do not use more than 4 layers and 64 hidden units unless you have access to sufficient compute).\n",
    "2. Initialise the dataloader using the dataset class from practical 4.\n",
    "3. Initialise the loss function (cross entropy loss), optimiser and learning rate scheduler.\n",
    "4. Implement the training loop.\n",
    "5. Train the model for 5 epochs and ensure that loss decreases for both the training and validation sets of the dataset. You can use a small randomly selected subset of the training data to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c8e9c54ce28429",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from dataset import get_costum_dataset\n",
    "from modelling.model import Transformer\n",
    "from modelling.lr_scheduler import TransformerLRScheduler\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from dataset import get_costum_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02e1b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since wmt17 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'de-en' at /Users/leonmarkwart/.cache/huggingface/datasets/wmt17/de-en/0.0.0/54d3aacfb5429020b9b85b170a677e4bc92f2449 (last modified on Thu Jan 16 01:41:05 2025).\n",
      "Using the latest cached version of the dataset since wmt17 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'de-en' at /Users/leonmarkwart/.cache/huggingface/datasets/wmt17/de-en/0.0.0/54d3aacfb5429020b9b85b170a677e4bc92f2449 (last modified on Thu Jan 16 01:41:05 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871399\n",
      "543\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=10_000, \n",
    "    d_model=512, \n",
    "    n_heads=8, \n",
    "    num_encoder_layers=4, \n",
    "    num_decoder_layers=4, \n",
    "    dim_feedforward=2048, \n",
    "    dropout=0.1, \n",
    "    max_len=64\n",
    ")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\"\"\"no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=1e-3)\n",
    "scheduler = TransformerLRScheduler(optimizer, d_model=512, warmup_steps=100)\"\"\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"modelling/bpe_v=10000_l=64\")\n",
    "\n",
    "ds_train = get_costum_dataset(split='train', tokenize=True) # 871_399 samples\n",
    "ds_val = get_costum_dataset(split='validation', tokenize=True) # 543 samples\n",
    "\n",
    "print(len(ds_train))\n",
    "print(len(ds_val))\n",
    "\n",
    "#build subsets\n",
    "ds_train = torch.utils.data.Subset(ds_train, range(100_000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d60cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['translation', 'src_input', 'tgt_input', 'tgt_output'])\n",
      "Sie w√ºrde es erlauben, gegen Urteile Einspruch zu erheben. [EOS] [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[BOS] This would allow for appeals to be made against judgements. [EOS] [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "This would allow for appeals to be made against judgements. [EOS] [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "sample = ds_val[50]\n",
    "print(sample.keys())\n",
    "print(tokenizer.decode(sample['src_input']))\n",
    "print(tokenizer.decode(sample['tgt_input']))\n",
    "print(tokenizer.decode(sample['tgt_output']))\n",
    "\n",
    "def collate_fn(sample):\n",
    "    src_input = [s['src_input'] for s in sample]\n",
    "    tgt_input = [s['tgt_input'] for s in sample]\n",
    "    tgt_output = [s['tgt_output'] for s in sample]\n",
    "    return {\n",
    "        'src_input': torch.tensor(src_input),\n",
    "        'tgt_input': torch.tensor(tgt_input),\n",
    "        'tgt_output': torch.tensor(tgt_output)\n",
    "    }\n",
    "    \n",
    "train_loader = DataLoader(ds_train, batch_size=64, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader = DataLoader(ds_val, batch_size=64, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c161d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonmarkwart/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:375: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   7%|‚ñã         | 113/1562 [00:03<00:52, 27.79it/s, loss=6.72e+4]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   7%|‚ñã         | 107/1562 [00:04<00:57, 25.13it/s, loss=7.15e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m tgt_input_mask \u001b[38;5;241m=\u001b[39m (tgt_input \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src_input, tgt_input, src_input_mask, tgt_input_mask)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(tgt_output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:268\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n\u001b[0;32m--> 268\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    269\u001b[0m     src,\n\u001b[1;32m    270\u001b[0m     mask\u001b[38;5;241m=\u001b[39msrc_mask,\n\u001b[1;32m    271\u001b[0m     src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[1;32m    272\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    275\u001b[0m     tgt,\n\u001b[1;32m    276\u001b[0m     memory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:507\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    504\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 507\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[1;32m    508\u001b[0m         output,\n\u001b[1;32m    509\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[1;32m    510\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    511\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    515\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:900\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    896\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    899\u001b[0m         x\n\u001b[0;32m--> 900\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m    901\u001b[0m     )\n\u001b[1;32m    902\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:914\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    909\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    913\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 914\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    915\u001b[0m         x,\n\u001b[1;32m    916\u001b[0m         x,\n\u001b[1;32m    917\u001b[0m         x,\n\u001b[1;32m    918\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    919\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    920\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    921\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    922\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchshow import show\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "model = torch.nn.Transformer(d_model=64, nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=2048, dropout=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    #move model to device\n",
    "    model#.to('mps')\n",
    "    train_loss = 0\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch}\")\n",
    "    for i, batch in enumerate(pbar):\n",
    "\n",
    "        src_input = batch['src_input'].float()#.to('mps')\n",
    "        tgt_input = batch['tgt_input'].float()#.to('mps')\n",
    "        tgt_output = batch['tgt_output'].float()#.to('mps') # [B, max_len]\n",
    "\n",
    "        #mask padding\n",
    "        src_input_mask = (src_input != 0).float()\n",
    "        tgt_input_mask = (tgt_input != 0).float()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_input, tgt_input, src_input_mask, tgt_input_mask)\n",
    "        print(output.shape)\n",
    "        print(tgt_output.shape)\n",
    "        loss_step = loss(output, tgt_output)\n",
    "        #loss_step = loss(output.view(-1, output.size(-1)), tgt_output.view(-1)) # [B, max_len], [B, max_len, vocab_size]\n",
    "\n",
    "        loss_step.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        train_loss += loss_step.item()\n",
    "        pbar.set_postfix({'loss': loss_step.item()})\n",
    "        pbar.update()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            src_input = batch['src_input'].float()\n",
    "            tgt_input = batch['tgt_input'].float()\n",
    "            tgt_output = batch['tgt_output'].float()\n",
    "\n",
    "            src_input_mask = (src_input != 0).float()\n",
    "            tgt_input_mask = (tgt_input != 0).float()\n",
    "\n",
    "            output = model(src_input, tgt_input, src_input_mask, tgt_input_mask)\n",
    "            loss_step = loss(output, tgt_output)\n",
    "            val_loss += loss_step.item()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]}, Validation Loss = {val_losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58ed02d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/leonmarkwart/miniconda3/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | Transformer      | 4.2 M  | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.933    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd61ba71a0a4fe0bf73f372898930e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonmarkwart/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [64, 10000], got [64, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 95\u001b[0m\n\u001b[1;32m     88\u001b[0m transformer \u001b[38;5;241m=\u001b[39m TransformerModel()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#tuner = pl.tuner.tuning.Tuner(trainer)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#lr_finder = tuner.lr_find(transformer)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#fig = lr_finder.plot(suggest=True)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#plt.savefig('lr_finder.png')\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(transformer)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    545\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1057\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39mstep_args)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[16], line 62\u001b[0m, in \u001b[0;36mTransformerModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     58\u001b[0m tgt_output \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;66;03m#.float()\u001b[39;00m\n\u001b[1;32m     60\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(src_input, tgt_input)\n\u001b[0;32m---> 62\u001b[0m loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(output, tgt_output)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_step, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_step\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1295\u001b[0m         target,\n\u001b[1;32m   1296\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1297\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[1;32m   1298\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1299\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[1;32m   1300\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:3489\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3488\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[1;32m   3490\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3491\u001b[0m     target,\n\u001b[1;32m   3492\u001b[0m     weight,\n\u001b[1;32m   3493\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[1;32m   3494\u001b[0m     ignore_index,\n\u001b[1;32m   3495\u001b[0m     label_smoothing,\n\u001b[1;32m   3496\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [64, 10000], got [64, 64]"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class TransformerModel(pl.LightningModule):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Transformer(\n",
    "            vocab_size=10_000, \n",
    "            d_model=64, \n",
    "            n_heads=8, \n",
    "            num_encoder_layers=4, \n",
    "            num_decoder_layers=4, \n",
    "            dim_feedforward=2048, \n",
    "            dropout=0.1, \n",
    "            max_len=64\n",
    "        )\n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        #self.scheduler = scheduler\n",
    "        self.lr = 1e-3\n",
    "\n",
    "        #self.last_feed_forward = self.model.head[0].weight.detach().clone()\n",
    "\n",
    "    def forward(self, src_input, tgt_input):\n",
    "        src_input_mask = (src_input != 0)#.float()\n",
    "        tgt_input_mask = (tgt_input != 0)#.float()\n",
    "        return self.model(src_input, tgt_input, src_input_mask, tgt_input_mask)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src_input = batch['src_input']#.float()\n",
    "        tgt_input = batch['tgt_input']#.float()\n",
    "        tgt_output = batch['tgt_output']#.float()\n",
    "\n",
    "        output = self(src_input, tgt_input)\n",
    "\n",
    "        print(output.shape)\n",
    "        print(tgt_output.shape)\n",
    "        loss_step = self.loss(output.long(), tgt_output.long())  # ((B, max_len), (B, max_len))\n",
    "        self.log('train_loss', loss_step, prog_bar=True, on_epoch=False, on_step=True, logger=True)\n",
    "        self.log('lr', self.lr, prog_bar=True, on_epoch=False, on_step=True, logger=True)\n",
    "        \n",
    "\n",
    "        return loss_step\n",
    "    \n",
    "    '''def on_after_backward(self):\n",
    "        global_step = self.global_step\n",
    "        for name, param in self.model.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, param, global_step)\n",
    "            if param.requires_grad:\n",
    "                self.logger.experiment.add_histogram(f\"{name}_grad\", param.grad, global_step)\n",
    "            else:\n",
    "                print(f\"NO GRADIENTS FOR {name}\")'''\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src_input = batch['src_input']#.float()\n",
    "        tgt_input = batch['tgt_input']#.float()\n",
    "        tgt_output = batch['tgt_output']#.float()\n",
    "\n",
    "        output = self(src_input, tgt_input)\n",
    "\n",
    "        loss_step = self.loss(output, tgt_output)\n",
    "        self.log('val_loss', loss_step, prog_bar=True, on_epoch=True, on_step=False, logger=True)\n",
    "        return loss_step\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\"\"\"\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=0.0)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "    \n",
    "trainer = pl.Trainer(\n",
    "    overfit_batches=100, \n",
    "    enable_checkpointing=False,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[pl.callbacks.LearningRateMonitor(logging_interval='step')],\n",
    "    \n",
    "    )\n",
    "transformer = TransformerModel()\n",
    "\n",
    "#tuner = pl.tuner.tuning.Tuner(trainer)\n",
    "#lr_finder = tuner.lr_find(transformer)\n",
    "#fig = lr_finder.plot(suggest=True)\n",
    "#plt.savefig('lr_finder.png')\n",
    "\n",
    "trainer.fit(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034297fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tokenizer.encode(\"Hallo, ich hei√üe Leon und bin 27 Jahre alt.[EOS]\", max_length=64, truncation=True, padding='max_length')\n",
    "tgt_input = tokenizer.encode(\"Hello, my name is Leon and i am 27\", max_length=64, truncation=True, padding='max_length')\n",
    "tgt_output = tokenizer.encode(\"[BOS]Hello, my name is Leon and i am 27 years\", max_length=64, truncation=True, padding='max_length')\n",
    "print(src_input)\n",
    "print(tokenizer.decode(src_input))\n",
    "print(tgt_input)\n",
    "print(tokenizer.decode(tgt_input))\n",
    "print(tgt_output)\n",
    "print(tokenizer.decode(tgt_output))\n",
    "\n",
    "src_input = torch.tensor(src_input).unsqueeze(0).to('mps')\n",
    "tgt_input = torch.tensor(tgt_input).unsqueeze(0).to('mps')\n",
    "tgt_output = torch.tensor(tgt_output).unsqueeze(0).to('mps')\n",
    "\n",
    "output = model(src_input, tgt_input)\n",
    "output = output.argmax(dim=-1)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
